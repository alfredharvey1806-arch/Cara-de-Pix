# BOOTSTRAP.md â€“ Setup Inicial para Economia de Tokens

## ğŸ“Š Objetivo
Reduzir context size de **~50KB â†’ 2-8KB** + Heartbeat grÃ¡tis via Ollama

## âœ… Checklist

### 1. Session Initialization (CONCLUÃDO)
- [x] SOUL.md enxuto
- [x] USER.md enxuto
- [x] IDENTITY.md enxuto
- [x] memory/2026-02-21.md criado
- [x] Carrega apenas esses 4 arquivos por padrÃ£o

### 2. Model Routing (CONCLUÃDO)
- [x] openclaw.json com Haiku default
- [x] Aliases "haiku" e "sonnet" configurados
- [x] Model Selection Rule no AGENTS.md

### 3. Heartbeat Ollama (PENDENTE)
**VocÃª precisa fazer isso:**

```bash
# 1. Instalar Ollama (macOS/Linux)
# macOS:
brew install ollama

# Linux:
curl -fsSL https://ollama.ai/install.sh | sh

# 2. Puxar o modelo leve
ollama pull llama3.2:3b

# 3. Testar (em outro terminal)
ollama serve

# 4. Em outro terminal, validar:
ollama run llama3.2:3b "respond with OK"
```

Depois volta aqui que valida tudo.

### 4. Rate Limits + Budgets (PENDENTE)
VocÃª precisa colocar no seu workflow:
- 5s mÃ­nimo entre chamadas
- 10s entre buscas
- MÃ¡x 5 buscas por batch
- Daily: $5 warning em 75%
- Monthly: $200 warning em 75%

### 5. Prompt Caching (CONCLUÃDO EM CONFIG)
- [x] Cache habilitado no openclaw.json (ttl 5m)
- [x] Estrutura de arquivos pronta:
  - EstÃ¡vel (SOUL, USER, IDENTITY, projetos): cacheÃ¡vel
  - DinÃ¢mico (memory/, tool outputs): nÃ£o cachear

### 6. ValidaÃ§Ã£o Final
```bash
# Rodar quando Ollama estiver pronto:
openclaw shell
> session_status

# VocÃª deve ver:
# - Context: 2-8KB (era ~50KB)
# - Default Model: haiku
# - Heartbeat: ollama/llama3.2:3b (local, sem API)
```

## ğŸ“ Notas
- Arquivo criado: 2026-02-21 10:35 GMT-3
- prÃ³ximo refresh: quando Ollama subir
